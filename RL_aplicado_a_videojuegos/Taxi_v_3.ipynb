{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taxi v-3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Taxi v-3 "
      ],
      "metadata": {
        "id": "zZ9tMWvivUhq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f8bBT5nMvOLd"
      },
      "outputs": [],
      "source": [
        "#Hacemos los imports\n",
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el ambiente, un espacio de 5x5, donde el taxi spawnea aleatoriamente, el pasajero se va a ubicar\n",
        "# en una de cuatro posiciones R, G, B o Y \n",
        "# y va a ir hacia una de esas posiciones también.\n",
        "\n",
        "ambiente = gym.make(\"Taxi-v3\")\n",
        "ambiente.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojpWhamvvnIJ",
        "outputId": "20a411c7-7299-4b9f-aef0-aeba4bdb37f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buscamos la cantidad de estados que va a tener el sistema (en este caso 500 = 25 * 4 * 5) tal que 25 es la cantidad\n",
        "# de posiciones en las que puede estar el taxi, 5 la cantidad de lugares donde puede estar el taxi (R,G,B,Y o sobre el taxi) y 4 por\n",
        "# la cantidad de lugares de destino.\n",
        "espacio_de_estados = ambiente.observation_space.n\n",
        "\n",
        "# Buscamos la cantidad de acciones posibles, en este caso será 6 (subir, bajar, izquierda, derecha, subir pasajero y bajar pasajero)\n",
        "espacio_de_acciones = ambiente.action_space.n\n",
        "\n",
        "\n",
        "#Comprobamos con prints\n",
        "print(\"Hay \", espacio_de_estados, \" estados posibles\")\n",
        "print(\"Hay \", espacio_de_acciones, \" acciones posibles\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YI0NQq6wcoo",
        "outputId": "6bf261b9-2878-4721-ede2-b003a5903961"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hay  500  estados posibles\n",
            "Hay  6  acciones posibles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la tabla Q, y la inicializamos en 0:\n",
        "\n",
        "tabla_Q = np.zeros((espacio_de_estados,espacio_de_acciones))\n",
        "print(tabla_Q)\n",
        "print(tabla_Q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbP_e6otx9m1",
        "outputId": "6ae927fd-cd5a-4847-c60a-69d3ac56e1dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "(500, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos los hiperparámetros\n",
        "cantidad_episodios_de_entrenamiento = 25000\n",
        "cantidad_episodios_de_test = 5\n",
        "maxima_cantidad_de_pasos = 200\n",
        "\n",
        "tasa_de_aprendizaje = 0.01\n",
        "gamma = 0.99                                    # Tasa de descuento\n",
        "\n",
        "# Parámetros de exploración\n",
        "epsilon = 1.0                                   # Tasa de exploración\n",
        "max_epsilon = 1.0                               # Exploración al principio\n",
        "min_epsilon = 0.001                             # Minima probabilidad de exploracion posible\n",
        "decay_rate = 0.01                               # Tasa de decaimiento exponencial para la exploración\n",
        "\n"
      ],
      "metadata": {
        "id": "xy2Pm6RSyrI3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la política epsilon-greedy (va a ser nuestra política de acción a realizar)\n",
        "\n",
        "def politica_epsilon_greedy(tabla_Q, estado):\n",
        "  # Si el valor aleatorio generado es mayor que épsilon -> Hago explotación\n",
        "  if(random.uniform(0,1) > epsilon):\n",
        "    action = np.argmax(tabla_Q[estado])\n",
        "  # Si no -> Hago exploración\n",
        "  else:\n",
        "    action = ambiente.action_space.sample()\n",
        "\n",
        "  return action"
      ],
      "metadata": {
        "id": "AJK_u_nnzr0l"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el algoritmo de Q-learning y entrenamos al agente\n",
        "\n",
        "for episodio in range(cantidad_episodios_de_entrenamiento):\n",
        "  # Reseteamos el ambiente\n",
        "  estado = ambiente.reset()\n",
        "  step = 0\n",
        "  terminado = False\n",
        "\n",
        "  # Reducimos el épsilon (para tener cada vez menos exploracion)\n",
        "  epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episodio)\n",
        "\n",
        "  for step in range(maxima_cantidad_de_pasos):\n",
        "    # Defino la accion a realizar\n",
        "    accion = politica_epsilon_greedy(tabla_Q,estado)\n",
        "\n",
        "    # Realizo la accion y analizo, la recompensa y el estado actual\n",
        "    nuevo_estado, recompensa, terminado, info = ambiente.step(accion)\n",
        "\n",
        "    # Actualizo la tabla Q\n",
        "    tabla_Q[estado][accion] = tabla_Q[estado][accion] + tasa_de_aprendizaje * (recompensa + gamma * np.max(tabla_Q[nuevo_estado]) - tabla_Q[estado][accion])\n",
        "\n",
        "    # Si terminé : finalizo el episodio\n",
        "    if terminado == True:\n",
        "      break\n",
        "\n",
        "    # Actualizo el estado\n",
        "    estado = nuevo_estado"
      ],
      "metadata": {
        "id": "cNv7vdhd1Vws"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Terminado el entrenamiento, procedemos a ejecutar los tests\n",
        "import time\n",
        "recompensas = []\n",
        "\n",
        "frames = []\n",
        "for episodio in range(cantidad_episodios_de_test):\n",
        "  estado = ambiente.reset()\n",
        "  step = 0\n",
        "  terminado = False\n",
        "  recompensa_total = 0\n",
        "  print(\"****************************************************\")\n",
        "  print(\"EPISODIO \", episodio)\n",
        "\n",
        "  for step in range(maxima_cantidad_de_pasos):\n",
        "    ambiente.render()\n",
        "    time.sleep(1)\n",
        "\n",
        "    accion = politica_epsilon_greedy(tabla_Q,estado)\n",
        "    nuevo_estado, recompensa, terminado, info = ambiente.step(accion)\n",
        "    recompensa_total += recompensa\n",
        "\n",
        "    if terminado:\n",
        "      recompensas.append(recompensa_total)\n",
        "      break\n",
        "    \n",
        "    estado = nuevo_estado\n",
        "    \n",
        "ambiente.close()\n",
        "print(\"Recompensas obtenidas: \" + str(sum(recompensas)/cantidad_episodios_de_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuHAOYsH5TLf",
        "outputId": "b42ccbd8-34a6-4b27-ccd0-145e654dee6f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************\n",
            "EPISODIO  0\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "Recompensas obtenidas: 5.0\n"
          ]
        }
      ]
    }
  ]
}