{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acciones posibles del agente:\n",
    "    #1.mover hacia arriba\n",
    "    #2.mover hacia abajo\n",
    "\n",
    "#Reglas del juego:\n",
    "    #El agente tiene 3 vidas.\n",
    "    #Si pierde… castigo, restamos 10 puntos.\n",
    "    #Cada vez que le demos a la bola, recompensa, sumamos 10.\n",
    "    #Para que no quede jugando por siempre, limitaremos el juego a\n",
    "        #3000 iteraciones máximo ó\n",
    "        #alcanzar 1000 puntos y habremos ganado.\n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import math #import ceil,floor\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de la clase *Agente* se encuentra la tabla donde se almacenaran las políticas. En este caso la tabla cuenta de 3 coordenadas:\n",
    "\n",
    "    - La posición actual del jugador.\n",
    "    - La posición “y” de la pelota.\n",
    "    - La posición en el eje “x” de la pelota.\n",
    "\n",
    "Además en esta clase, se define el factor de descuento, el learning rate y el ratio de exploración.\n",
    "\n",
    "Métodos más importantes:\n",
    "\n",
    "    * get_next_step() decide la siguiente acción a tomar en base al ratio de exploración si tomar “el mejor paso” que tuviéra almacenado ó tomar un paso al azar, dando posibilidad a explorar el ambiente\n",
    "    * update() aquí se actualizan las políticas mediante la ecuación de Bellman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongAgent:\n",
    "    \n",
    "    def __init__(self, game, policy=None, discount_factor = 0.1, learning_rate = 0.1, ratio_explotacion = 0.9):\n",
    "\n",
    "        # Creamos la tabla de politicas\n",
    "        if policy is not None:\n",
    "            self._q_table = policy\n",
    "        else:\n",
    "            position = list(game.positions_space.shape)\n",
    "            position.append(len(game.action_space))\n",
    "            self._q_table = np.zeros(position)\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ratio_explotacion = ratio_explotacion\n",
    "\n",
    "    def get_next_step(self, state, game):\n",
    "        #decide la siguiente acción a tomar en base al ratio de exploración si tomar “el mejor paso” \n",
    "        #que tuviéramos almacenado ó tomar un paso al azar, dando posibilidad a explorar el ambiente\n",
    "        \n",
    "        # Damos un paso aleatorio...\n",
    "        next_step = np.random.choice(list(game.action_space))\n",
    "        \n",
    "        # o tomaremos el mejor paso...\n",
    "        if np.random.uniform() <= self.ratio_explotacion:\n",
    "            # tomar el maximo\n",
    "            idx_action = np.random.choice(np.flatnonzero(\n",
    "                    self._q_table[state[0],state[1],state[2]] == self._q_table[state[0],state[1],state[2]].max()\n",
    "                ))\n",
    "            next_step = list(game.action_space)[idx_action]\n",
    "\n",
    "        return next_step\n",
    "\n",
    "    # actualizamos las politicas con las recompensas obtenidas mediante la ecuación de Bellman\n",
    "    def update(self, game, old_state, action_taken, reward_action_taken, new_state, reached_end):\n",
    "        idx_action_taken =list(game.action_space).index(action_taken)\n",
    "\n",
    "        actual_q_value_options = self._q_table[old_state[0], old_state[1], old_state[2]]\n",
    "        actual_q_value = actual_q_value_options[idx_action_taken]\n",
    "\n",
    "        future_q_value_options = self._q_table[new_state[0], new_state[1], new_state[2]]\n",
    "        future_max_q_value = reward_action_taken  +  self.discount_factor*future_q_value_options.max()\n",
    "        if reached_end:\n",
    "            future_max_q_value = reward_action_taken #maximum reward\n",
    "\n",
    "        self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken] = actual_q_value + \\\n",
    "                                              self.learning_rate*(future_max_q_value -actual_q_value)\n",
    "    \n",
    "    def print_policy(self):\n",
    "        for row in np.round(self._q_table,1):\n",
    "            for column in row:\n",
    "                print('[', end='')\n",
    "                for value in column:\n",
    "                    print(str(value).zfill(5), end=' ')\n",
    "                print('] ', end='')\n",
    "            print('')\n",
    "            \n",
    "    def get_policy(self):\n",
    "        return self._q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la clase de *Ambiente* se implementa la lógica y control del juego del pong. Se controla que la pelotita rebote, que no se salga de la pantalla y se encuentran los métodos para graficar y animar en matplotlib.\n",
    "\n",
    "Por Defecto se define una pantalla de 40 pixeles x 50px de alto y si utilizamos la variable “movimiento_px = 5” quedará definida la tabla de políticas en 8 de alto y 10 de ancho (por hacer 40/5=8 y 50/5=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongEnvironment:\n",
    "    \n",
    "    def __init__(self, max_life=3, height_px = 40, width_px = 50, movimiento_px = 3):\n",
    "        \n",
    "        self.action_space = ['Arriba','Abajo']\n",
    "        \n",
    "        self._step_penalization = 0\n",
    "        \n",
    "        self.state = [0,0,0]\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        \n",
    "        self.dx = movimiento_px\n",
    "        self.dy = movimiento_px\n",
    "        \n",
    "        filas = math.ceil(height_px/movimiento_px)\n",
    "        columnas = math.ceil(width_px/movimiento_px)\n",
    "        \n",
    "        self.positions_space = np.array([[[0 for z in range(columnas)] \n",
    "                                                  for y in range(filas)] \n",
    "                                                     for x in range(filas)])\n",
    "\n",
    "        self.lives = max_life\n",
    "        self.max_life=max_life\n",
    "        \n",
    "        self.x = randint(int(width_px/2), width_px) \n",
    "        self.y = randint(0, height_px-10)\n",
    "        \n",
    "        self.player_alto = int(height_px/4)\n",
    "\n",
    "        self.player1 = self.player_alto  # posic. inicial del player\n",
    "        \n",
    "        self.score = 0\n",
    "        \n",
    "        self.width_px = width_px\n",
    "        self.height_px = height_px\n",
    "        self.radio = 2.5\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.state = [0,0,0]\n",
    "        self.lives = self.max_life\n",
    "        self.score = 0\n",
    "        self.x = randint(int(self.width_px/2), self.width_px) \n",
    "        self.y = randint(0, self.height_px-10)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action, animate=False):\n",
    "        self._apply_action(action, animate)\n",
    "        done = self.lives <=0 # final\n",
    "        reward = self.score\n",
    "        reward += self._step_penalization\n",
    "        self.total_reward += reward\n",
    "        return self.state, reward , done\n",
    "\n",
    "    def _apply_action(self, action, animate=False):\n",
    "        \n",
    "        if action == \"Arriba\":\n",
    "            self.player1 += abs(self.dy)\n",
    "        elif action == \"Abajo\":\n",
    "            self.player1 -= abs(self.dy)\n",
    "            \n",
    "        self.avanza_player()\n",
    "\n",
    "        self.avanza_frame()\n",
    "\n",
    "        if animate:\n",
    "            clear_output(wait=True);\n",
    "            fig = self.dibujar_frame()\n",
    "            plt.show()\n",
    "\n",
    "        self.state = (floor(self.player1/abs(self.dy))-2, floor(self.y/abs(self.dy))-2, floor(self.x/abs(self.dx))-2)\n",
    "    \n",
    "    def detectaColision(self, ball_y, player_y):\n",
    "        if (player_y+self.player_alto >= (ball_y-self.radio)) and (player_y <= (ball_y+self.radio)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def avanza_player(self):\n",
    "        if self.player1 + self.player_alto >= self.height_px:\n",
    "            self.player1 = self.height_px - self.player_alto\n",
    "        elif self.player1 <= -abs(self.dy):\n",
    "            self.player1 = -abs(self.dy)\n",
    "\n",
    "    def avanza_frame(self):\n",
    "        self.x += self.dx\n",
    "        self.y += self.dy\n",
    "        if self.x <= 3 or self.x > self.width_px:\n",
    "            self.dx = -self.dx\n",
    "            if self.x <= 3:\n",
    "                ret = self.detectaColision(self.y, self.player1)\n",
    "\n",
    "                if ret:\n",
    "                    self.score = 10\n",
    "                else:\n",
    "                    self.score = -10\n",
    "                    self.lives -= 1\n",
    "                    if self.lives>0:\n",
    "                        self.x = randint(int(self.width_px/2), self.width_px)\n",
    "                        self.y = randint(0, self.height_px-10)\n",
    "                        self.dx = abs(self.dx)\n",
    "                        self.dy = abs(self.dy)\n",
    "        else:\n",
    "            self.score = 0\n",
    "\n",
    "        if self.y < 0 or self.y > self.height_px:\n",
    "            self.dy = -self.dy\n",
    "\n",
    "    def dibujar_frame(self):\n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        a1 = plt.gca()\n",
    "        circle = plt.Circle((self.x, self.y), self.radio, fc='slategray', ec=\"black\")\n",
    "        a1.set_ylim(-5, self.height_px+5)\n",
    "        a1.set_xlim(-5, self.width_px+5)\n",
    "\n",
    "        rectangle = plt.Rectangle((-5, self.player1), 5, self.player_alto, fc='gold', ec=\"none\")\n",
    "        a1.add_patch(circle);\n",
    "        a1.add_patch(rectangle)\n",
    "        #a1.set_yticklabels([]);a1.set_xticklabels([]);\n",
    "        plt.text(4, self.height_px, \"SCORE:\"+str(self.total_reward)+\"  LIFE:\"+str(self.lives), fontsize=12)\n",
    "        if self.lives <=0:\n",
    "            plt.text(10, self.height_px-14, \"GAME OVER\", fontsize=16)\n",
    "        elif self.total_reward >= 1000:\n",
    "            plt.text(10, self.height_px-14, \"YOU WIN!\", fontsize=16)\n",
    "        return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se define una función para jugar, donde es necesario indicar la cantidad de veces que se quiere iterar la simulación del juego y se ira almacenando algunas estadísticas sobre el comportamiento del agente, si mejora el puntaje con las iteraciones y el máximo puntaje alcanzado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(rounds=5000, max_life=3, discount_factor = 0.1, learning_rate = 0.1,\n",
    "         ratio_explotacion=0.9,learner=None, game=None, animate=False):\n",
    "\n",
    "    if game is None:\n",
    "        # si usamos movimiento_px = 5 creamos una tabla de politicas de 8x10\n",
    "        # si usamos movimiento_px = 3 la tabla sera de 14x17\n",
    "        game = PongEnvironment(max_life=max_life, movimiento_px = 3)\n",
    "        \n",
    "    if learner is None:\n",
    "        print(\"Begin new Train!\")\n",
    "        learner = PongAgent(game, discount_factor = discount_factor,learning_rate = learning_rate, ratio_explotacion= ratio_explotacion)\n",
    "\n",
    "    max_points= -9999\n",
    "    first_max_reached = 0\n",
    "    total_rw=0\n",
    "    steps=[]\n",
    "\n",
    "    for played_games in range(0, rounds):\n",
    "        state = game.reset()\n",
    "        reward, done = None, None\n",
    "        \n",
    "        itera=0\n",
    "        while (done != True) and (itera < 3000 and game.total_reward<=1000):\n",
    "            old_state = np.array(state)\n",
    "            next_action = learner.get_next_step(state, game)\n",
    "            state, reward, done = game.step(next_action, animate=animate)\n",
    "            if rounds > 1:\n",
    "                learner.update(game, old_state, next_action, reward, state, done)\n",
    "            itera+=1\n",
    "        \n",
    "        steps.append(itera)\n",
    "        \n",
    "        total_rw+=game.total_reward\n",
    "        if game.total_reward > max_points:\n",
    "            max_points=game.total_reward\n",
    "            first_max_reached = played_games\n",
    "        \n",
    "        if played_games %500==0 and played_games >1 and not animate:\n",
    "            print(\"-- Partidas[\", played_games, \"] Avg.Puntos[\", int(total_rw/played_games),\"]  AVG Steps[\", int(np.array(steps).mean()), \"] Max Score[\", max_points,\"]\")\n",
    "                \n",
    "    if played_games>1:\n",
    "        print('Partidas[',played_games,'] Avg.Puntos[',int(total_rw/played_games),'] Max score[', max_points,'] en partida[',first_max_reached,']')\n",
    "        \n",
    "    #learner.print_policy()\n",
    "    \n",
    "    return learner, game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar ejecutamos la función con los siguientes parámetros:\n",
    "\n",
    "    * 6000 partidas de entrenamiendo\n",
    "    * ratio de explotación: el 85% de las veces será avaro, pero el 15% elige acciones aleatorias, dando lugar a la exploración.\n",
    "    * learning rate = se suele dejar en el 10 por ciento como un valor razonable, dando lugar a las recompensas y permitiendo actualizar la importancia de cada acción poco a poco. Tras más iteraciones, mayor importancia tendrá esa acción.\n",
    "    * discount_factor = También se suele empezar con valor de 0.1 pero aquí utilizamos un valor del 0.2 para intentar indicar al algoritmo que nos interesa las recompensas a más largo plazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin new Train!\n",
      "-- Partidas[ 500 ] Avg.Puntos[ 4 ]  AVG Steps[ 187 ] Max Score[ 150 ]\n",
      "-- Partidas[ 1000 ] Avg.Puntos[ 15 ]  AVG Steps[ 225 ] Max Score[ 210 ]\n",
      "-- Partidas[ 1500 ] Avg.Puntos[ 21 ]  AVG Steps[ 242 ] Max Score[ 210 ]\n",
      "-- Partidas[ 2000 ] Avg.Puntos[ 24 ]  AVG Steps[ 254 ] Max Score[ 320 ]\n",
      "-- Partidas[ 2500 ] Avg.Puntos[ 26 ]  AVG Steps[ 261 ] Max Score[ 320 ]\n",
      "-- Partidas[ 3000 ] Avg.Puntos[ 30 ]  AVG Steps[ 273 ] Max Score[ 320 ]\n",
      "-- Partidas[ 3500 ] Avg.Puntos[ 32 ]  AVG Steps[ 281 ] Max Score[ 320 ]\n",
      "-- Partidas[ 4000 ] Avg.Puntos[ 34 ]  AVG Steps[ 286 ] Max Score[ 320 ]\n",
      "-- Partidas[ 4500 ] Avg.Puntos[ 34 ]  AVG Steps[ 288 ] Max Score[ 320 ]\n",
      "Partidas[ 4999 ] Avg.Puntos[ 35 ] Max score[ 320 ] en partida[ 1531 ]\n"
     ]
    }
   ],
   "source": [
    "learner, game = play(rounds=5000, discount_factor = 0.2, learning_rate = 0.1, ratio_explotacion=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner2 = PongAgent(game, policy=learner.get_policy())\n",
    "learner2.ratio_explotacion = 1.0  # con esto quitamos las elecciones aleatorias al jugar\n",
    "player = play(rounds=1, learner=learner2, game=game, animate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
