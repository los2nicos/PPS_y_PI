{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Micocubo_v_0_0_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Para abrir en colab:\n",
        "\n",
        "https://colab.research.google.com/drive/1sSu2kvUld4njCbIfFwV9SmqDSHQnOSk3?usp=sharing"
      ],
      "metadata": {
        "id": "JNwXUGiKKC_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p0Hg8qEgVYWJ"
      },
      "outputs": [],
      "source": [
        "# Realizamos los imports necesarios\n",
        "import numpy as np\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import matplotlib, cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import base64, io, os, time, gym\n",
        "import IPython, functools\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos la red como ambiente"
      ],
      "metadata": {
        "id": "A4YzTuwFVmJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pn_environ:\n",
        "    \n",
        "    def __init__(self,init_state,action_space,W_pre,W_post):\n",
        "        # Estados\n",
        "        self.state = init_state\n",
        "        self.init_state = init_state\n",
        "        # Transiciones\n",
        "        self.action_space = action_space\n",
        "        self.T_mapped = {T:n for n,T in zip(range(len(self.action_space)), self.action_space)}\n",
        "        # Matrices de incidencia\n",
        "        self.W_pre = W_pre\n",
        "        self.W_post= W_post\n",
        "        self.W    = self.W_post - self.W_pre\n",
        "        # Actualizamos la sensibilizacion por marcado\n",
        "        self.T_shooteable_by_marked = [False for i in action_space]\n",
        "        self.update_T_by_marks()\n",
        "        # Sobre invariantes\n",
        "        self.n_I1 = 1\n",
        "        self.n_I2 = 1\n",
        "\n",
        "    def update_T_by_marks(self):\n",
        "        for i in range(len(self.action_space)):\n",
        "          action_idx = i\n",
        "          marks_after_W_pre = self.state - self.W_pre[:,action_idx]\n",
        "          self.T_shooteable_by_marked[action_idx] = False if any(n_tok < 0 for n_tok in marks_after_W_pre) else True\n",
        "\n",
        "    def map_T(self):\n",
        "        self.T_mapped = {T:n for n, T in self.action_space}\n",
        "\n",
        "    def can_shoot(self,action):\n",
        "        return self.T_shooteable_by_marked[action]\n",
        "\n",
        "    def shoot_pn(self,action):\n",
        "        self.state = self.state + self.W[:,action]\n",
        "        self.update_T_by_marks()\n",
        "        return self.state\n",
        "\n",
        "    def step(self,action):\n",
        "        if self.can_shoot(action) is False:\n",
        "            self.shoot_pn(action)\n",
        "            reward = -100.0\n",
        "            done   = True\n",
        "            info   = None\n",
        "            state  = tf.constant(self.state)\n",
        "        else:\n",
        "            self.shoot_pn(action)\n",
        "            state = tf.constant(self.state)\n",
        "            reward = 2 + self.reward_by_resourse_used(action) + self.reward_by_invariant_completed(action)  \n",
        "            done   = False\n",
        "            info   = None\n",
        "        #print(\"Action was: \",action)\n",
        "        #print(\"Reward was: \",reward)\n",
        "        return state, reward, done, info \n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.init_state\n",
        "        self.update_T_by_marks()\n",
        "        self.n_I1 = 1\n",
        "        self.n_I2 = 1\n",
        "        return tf.constant(self.state) \n",
        "\n",
        "    def set(self, state, n_I1=1,n_I2=1):\n",
        "        self.state = state\n",
        "        self.update_T_by_marks()\n",
        "        self.n_I1 = n_I1\n",
        "        self.n_I2 = n_I2\n",
        "\n",
        "    def reward_by_resourse_used(self, action):\n",
        "        reward = 0\n",
        "        if action == 0 or action == 3:\n",
        "            reward += 5  \n",
        "        return reward\n",
        "    \n",
        "    def reward_by_invariant_completed(self, action):\n",
        "        reward = 0\n",
        "        if action == 2 or action == 5:\n",
        "          self.n_I1 += 1 if action == 2 else 0\n",
        "          self.n_I2 += 1 if action == 5 else 0\n",
        "          reward += 20\n",
        "\n",
        "          if self.n_I1 == self.n_I2:\n",
        "            reward += 100\n",
        "        \n",
        "        return reward"
      ],
      "metadata": {
        "id": "Ygp4Xpi3VkcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Red simple con mutex"
      ],
      "metadata": {
        "id": "1PWouAKaV6u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutamos para red simple con mutex\n",
        "action_space = [\"T0\",\"T1\",\"T2\",\"T3\",\"T4\",\"T5\"]\n",
        "init_state   = [1,0,0,1,0,0,1]\n",
        "W_pre = np.array([[1,0,0,0,0,0],\n",
        "                  [0,1,0,0,0,0],\n",
        "                  [0,0,1,0,0,0],\n",
        "                  [0,0,0,1,0,0],\n",
        "                  [0,0,0,0,1,0],\n",
        "                  [0,0,0,0,0,1],\n",
        "                  [1,0,0,1,0,0]])\n",
        "\n",
        "W_post= np.array([[0,0,1,0,0,0],\n",
        "                  [1,0,0,0,0,0],\n",
        "                  [0,1,0,0,0,0],\n",
        "                  [0,0,0,0,0,1],\n",
        "                  [0,0,0,1,0,0],\n",
        "                  [0,0,0,0,1,0],\n",
        "                  [0,1,0,0,1,0]])"
      ],
      "metadata": {
        "id": "jI0Jte38V6Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definimos la memoria del agente"
      ],
      "metadata": {
        "id": "xpOKL16LWN8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.clear()\n",
        "\n",
        "    #Limpia la memoria\n",
        "    def clear(self):\n",
        "        self.observations = []\n",
        "        self.actions      = []\n",
        "        self.rewards      = []\n",
        "\n",
        "    # AÃ±adimos una nueva observacion, accion y recompensa a la memoria\n",
        "    def add_to_memory(self,new_observation,new_action,new_reward):\n",
        "        self.observations.append(new_observation)\n",
        "        self.actions.append(new_action)\n",
        "        self.rewards.append(new_reward)"
      ],
      "metadata": {
        "id": "mGzjOLlIV8-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Empezamos el entrenamiento"
      ],
      "metadata": {
        "id": "GvAhkqvdWmz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos los parametros de aprendizaje\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "n_episodes_of_training = 20000\n",
        "n_max_steps = 120\n",
        "total_rewards = []\n",
        "environ = pn_environ(init_state,action_space,W_pre,W_post)\n",
        "n_tests=0\n",
        "\n",
        "memory = Memory()\n",
        "n_actions = len(environ.action_space)"
      ],
      "metadata": {
        "id": "rA2m7n2eWnEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definimos el cubo (tensor)"
      ],
      "metadata": {
        "id": "mosVH2u_Xhe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cubo = tf.zeros([n_actions, n_actions, n_actions])\n",
        "cubo = cubo.numpy()"
      ],
      "metadata": {
        "id": "0y13gE5uXhzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### Entrenamiento"
      ],
      "metadata": {
        "id": "dVY6h4NiWfir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pasamos el cubo para ver recompensas\n",
        "    \n",
        "for i_step in range(n_actions):\n",
        "    i_reward = 0\n",
        "    observation = environ.reset()\n",
        "    action = i_step\n",
        "    i_new_observation, i_reward, i_done, info = environ.step(action)\n",
        "      \n",
        "    for j_step in range(n_actions):\n",
        "      j_reward = 0\n",
        "      if i_done is False:\n",
        "        # Actualizamos el estado\n",
        "        environ.set(i_new_observation)\n",
        "        \n",
        "        action = j_step\n",
        "        j_new_observation, j_reward, j_done, info = environ.step(action)\n",
        "          \n",
        "      for k_step in range(n_actions):\n",
        "        k_reward = 0\n",
        "        if i_done is False and j_done is False:  \n",
        "          # Actualizamos el estado\n",
        "          environ.set(j_new_observation)\n",
        "\n",
        "          action = k_step\n",
        "          k_new_observation, k_reward, done, info = environ.step(action)\n",
        "        \n",
        "        cubo[i_step][j_step][k_step] = i_reward + j_reward + k_reward"
      ],
      "metadata": {
        "id": "zQHLTPEvWfxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imprimimos los resultados"
      ],
      "metadata": {
        "id": "znYobxUz2GDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max = np.max(cubo)\n",
        "paths = np.where(cubo == max)\n",
        "\n",
        "print(\"Mayor resultado: \", max)\n",
        "print(\"Siguiendo los caminos: \")\n",
        "\n",
        "string = \"\\t\\t\\t\"\n",
        "for i in range(len(paths[0])):\n",
        "  for j in range(len(paths)):\n",
        "    string+=str(paths[j][i])\n",
        "    string+=' - ' if j != len(paths)-1 else \"\\n\\t\\t\\t\"\n",
        "\n",
        "print(string)"
      ],
      "metadata": {
        "id": "UHSEgGXbW9ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ef36ee-e499-4c80-8f4a-510a1913eb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mayor resultado:  31.0\n",
            "Siguiendo los caminos: \n",
            "\t\t\t0 - 1 - 2\n",
            "\t\t\t3 - 4 - 5\n",
            "\t\t\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cubo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3alvtgG32O0k",
        "outputId": "52f263f0-3b2e-4ec1-e99c-af2424cdf35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ -93.  -93.  -93.  -93.  -93.  -93.]\n",
            "  [ -91.  -91.   31.   16.  -91.  -91.]\n",
            "  [ -93.  -93.  -93.  -93.  -93.  -93.]\n",
            "  [ -93.  -93.  -93.  -93.  -93.  -93.]\n",
            "  [ -93.  -93.  -93.  -93.  -93.  -93.]\n",
            "  [ -93.  -93.  -93.  -93.  -93.  -93.]]\n",
            "\n",
            " [[-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]]\n",
            "\n",
            " [[-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]]\n",
            "\n",
            " [[ -93.  -93.  -93.  -93.  -93.  -93.]\n",
            "  [ -93.  -93.  -93.  -93.  -93.  -93.]\n",
            "  [ -93.  -93.  -93.  -93.  -93.  -93.]\n",
            "  [ -93.  -93.  -93.  -93.  -93.  -93.]\n",
            "  [  16.  -91.  -91.  -91.  -91.   31.]\n",
            "  [ -93.  -93.  -93.  -93.  -93.  -93.]]\n",
            "\n",
            " [[-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]]\n",
            "\n",
            " [[-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]\n",
            "  [-100. -100. -100. -100. -100. -100.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-HC3X0l_5nVu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
